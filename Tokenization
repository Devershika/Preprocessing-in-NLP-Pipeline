#Using NLTK
import nltk
from nltk.tokenize import sent_tokenize, word_tokenize

nltk.download('punkt')
nltk.download('punkt_tab')
text = "Natural Language Processing is fascinating! It enables machines to understand human language."

# Sentence Tokenization
sentences = sent_tokenize(text)
print("Sentence-level Tokenization:")
print(sentences)

# Word Tokenization
words = word_tokenize(text)
print("\nWord-level Tokenization:")
print(words)


##Using Spacy
import spacy

# Load English tokenizer
nlp = spacy.load("en_core_web_sm")

text = "Natural Language Processing is fascinating! It enables machines to understand human language."
doc = nlp(text)

# Sentence Tokenization
sentences = [sent.text for sent in doc.sents]
print("Sentence-level Tokenization:")
print(sentences)

# Word Tokenization
words = [token.text for token in doc]
print("\nWord-level Tokenization:")
print(words)
