import torch
import torch.nn as nn
import torch.optim as optim
import torch.nn.functional as F
from nltk.corpus import gutenberg
from nltk.tokenize import word_tokenize
from collections import Counter
import nltk
import random

nltk.download('all')
#tokenize
raw_text = gutenberg.raw('austen-emma.txt')
tokens = word_tokenize(raw_text)

tokens[:10]

#create vocabulary
vocab = list(set(tokens))
word_to_ix = {word: i for i, word in enumerate(vocab)}
ix_to_word = {i: word for word, i in word_to_ix.items()}

def create_cbow_dataset(tokens, window_size=2):
  data = []
  for i in range(window_size, len(tokens) - window_size):
    context = [tokens[j] for j in range(i - window_size, i + window_size + 1) if j != i]
    target = tokens[i]
    data.append((context, target))
  return data[:100000]

data_trial = create_cbow_dataset(['today','is','a','pleasant','day','sir'])
print(data_trial)

class CBOWModel(nn.Module):
  def __init__(self, vocab_size, embedding_dim):
    super(CBOWModel, self).__init__()
    self.embeddings = nn.Embedding(vocab_size, embedding_dim)
    self.linear = nn.Linear(embedding_dim, vocab_size)

  def forward(self, context_idxs):
    embeds = self.embeddings(context_idxs)
    avg_embed = embeds.mean(dim=1)
    out = self.linear(avg_embed)
    return out

def train_cbow(model, data, epochs=5, lr=0.01):
  loss_fn = nn.CrossEntropyLoss()
  optimizer = optim.SGD(model.parameters(), lr=lr)

  for epoch in range(epochs):
    total_loss = 0
    random.shuffle(data)
    for context, target in data:
      context_idxs = torch.tensor([[word_to_ix[w] for w in context]], dtype=torch.long)
      target_idx = torch.tensor([word_to_ix[target]], dtype=torch.long)

      output = model(context_idxs)
      loss = loss_fn(output, target_idx)

      optimizer.zero_grad()
      loss.backward()
      optimizer.step()

      total_loss += loss.item()

    print(f"Epoch {epoch+1}/{epochs}, Loss: {total_loss: .2f}")

def cosine_similarity(vec1, vec2):
  vec1 = vec1 / (vec1.norm() + 1e-9)
  vec2 = vec2 / (vec2.norm() + 1e-9)
  return torch.dot(vec1, vec2).item()

def word_similarity(word1, word2, model):
  if word1 not in word_to_ix or word2 not in word_to_ix:
    return None
  idx1 = torch.tensor(word_to_ix[word1])
  idx2 = torch.tensor(word_to_ix[word2])
  vec1 = model.embeddings(idx1)
  vec2 = model.embeddings(idx2)
  return cosine_similarity(vec1, vec2)

embedding_dim = 100
cbow_model = CBOWModel(len(vocab), embedding_dim)
cbow_data = create_cbow_dataset(tokens, window_size=2)
train_cbow(cbow_model, cbow_data, epochs=10, lr=0.05)

print("\nSimilarity examples:")
print("love vs affection:", word_similarity("love", "affection", cbow_model))
print("mrs vs women:", word_similarity("mrs", "women", cbow_model))
print("book vs letter:", word_similarity("book", "letter", cbow_model))
